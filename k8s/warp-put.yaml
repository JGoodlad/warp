apiVersion: batch/v1
kind: Job
metadata:
  name: warp-job
spec:
  template:
    spec:
      nodeSelector:
        iam.gke.io/gke-metadata-server-enabled: "true"
      serviceAccountName: warp-benchmark
      containers:
      - name: warp-job
        env:
          - name: BENCHMARK_BUCKET_NAME
            value: put_numa_20m_64M_64n
          - name: DURATION
            value: "20m"
          - name: OBJECT_SIZE
            value: "64"
          - name: MAX_NODE_INDEX
            value: "63"
          - name: NUMA_NODE
            value: "1"
          - name: WARP_HOST
            value: "storage.googleapis.com"
          - name: WARP_TLS
            value: "true"
          - name: WARP_ACCESS_KEY
            value: "${WARP_ACCESS_KEY}"
          - name: WARP_SECRET_KEY
            value: "${WARP_SECRET_KEY}"
        image: "gcr.io/gcs-tess/warp-server:latest"
        imagePullPolicy: Always
        command:
          - /bin/sh
          - -c
          - |  
            echo "About to start benchmark nammed ${BENCHMARK_BUCKET_NAME}..."
            set -x
            . ./retry.sh
            COMMAND="numactl --cpunodebind=$NUMA_NODE --membind=$NUMA_NODE \
            ./warp \
              put \
              --bucket=gcs-tess-warp-goodlad-us-central1-01 \
              --concurrent=64 \
              --obj.size=${OBJECT_SIZE}MiB \
              --part.size=8MiB \
              --duration=${DURATION} \
              --warp-client=warp-{0...$MAX_NODE_INDEX}.warp.default.svc.cluster.local:7761"
            echo $COMMAND > ${BENCHMARK_BUCKET_NAME}_$(date +%s)_command.log
            mkfifo stdout_pipe stderr_pipe
            tee ${BENCHMARK_BUCKET_NAME}_$(date +%s)_stdout.log < stdout_pipe &
            tee ${BENCHMARK_BUCKET_NAME}_$(date +%s)_stderr.log < stderr_pipe &
            eval "$COMMAND" > stdout_pipe 2> stderr_pipe
            rm stdout_pipe stderr_pipe
            echo $(gcloud auth print-access-token) > token.txt
            rename 's/\[/-/; s/\]//' *.csv.zst
            BENCHMARK_FILENAME=$(find . -maxdepth 1 -type f -name "*.csv.zst" -printf "%f\n")
            ./warp analyze --analyze.v ${BENCHMARK_FILENAME} > ${BENCHMARK_BUCKET_NAME}_$(date +%s)_analyze.log 2>&1
            gcloud config set storage/parallel_composite_upload_enabled False
            gcloud storage --access-token-file=token.txt cp $BENCHMARK_FILENAME gs://gcs-tess-warp-benchmarks/20220726/${BENCHMARK_BUCKET_NAME}/
            gcloud storage --access-token-file=token.txt cp *.log gs://gcs-tess-warp-benchmarks/20220726/${BENCHMARK_BUCKET_NAME}/
            echo "Benchmark complete and results uploaded to gs://gcs-tess-warp-benchmarks/20220726/${BENCHMARK_BUCKET_NAME}"
      restartPolicy: Never
  backoffLimit: 1


# gsutil --use-external-account -m cp *.csv.zst gs://gcs-tess-warp-benchmarks/20220726/${BENCHMARK_BUCKET_NAME}/