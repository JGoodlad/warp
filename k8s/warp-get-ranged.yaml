apiVersion: batch/v1
kind: Job
metadata:
  name: warp-job
spec:
  template:
    spec:
      nodeSelector:
        iam.gke.io/gke-metadata-server-enabled: "true"
      serviceAccountName: warp-benchmark
      containers:
      - name: warp-job
        env:
          - name: BENCHMARK_BUCKET_PREFIX
            value: gs://gcs-tess-warp-benchmarks/${BUCKET_PREFIX}
          - name: BENCHMARK_BUCKET_NAME
            value: get_ranged_numa_${NODES}n_${SIZE}M_${THREADS}t_${DURATION_MIN}m
          - name: NUM_NODES
            value: "${NODES}"
          - name: NUM_THREADS
            value: "${THREADS}"
          - name: SIZE
            value: "${SIZE}"
          - name: DURATION
            value: "${DURATION_MIN}m"
          - name: NUMA_NODE
            value: "1"
          - name: WARP_HOST
            value: "storage.googleapis.com"
          - name: WARP_TLS
            value: "true"
          - name: WARP_ACCESS_KEY
            value: "${WARP_ACCESS_KEY}"
          - name: WARP_SECRET_KEY
            value: "${WARP_SECRET_KEY}"
        image: "gcr.io/gcs-tess/warp-server:latest"
        imagePullPolicy: Always
        command:
          - /bin/sh
          - -c
          - |
            echo "About to start the benchmark."
            
            set -x
            export BENCHMARK_BUCKET_PREFIX="${BENCHMARK_BUCKET_PREFIX}_$(date +%Y%m%d).$(date +%s)/"
            export MAX_NODE_INDEX=$((NUM_NODES - 1))

            COMMAND="numactl --cpunodebind=$NUMA_NODE --membind=$NUMA_NODE \
            ./warp \
              get \
              --bucket=gcs-tess-warp-goodlad-us-central1-01 \
              --concurrent=${NUM_THREADS} \
              --objects=64 \
              --obj.size=${SIZE}MiB \
              --range=true \
              --range-size=8MiB \
              --part.size=8MiB \
              --duration=${DURATION} \
              --warp-client=warp-{0...$MAX_NODE_INDEX}.warp.default.svc.cluster.local:7761"

            echo $COMMAND > ${BENCHMARK_BUCKET_NAME}_$(date +%s)_command.log

            mkfifo stdout_pipe stderr_pipe
            tee ${BENCHMARK_BUCKET_NAME}_$(date +%s)_stdout.log < stdout_pipe &
            tee ${BENCHMARK_BUCKET_NAME}_$(date +%s)_stderr.log < stderr_pipe &
            eval "$COMMAND" > stdout_pipe 2> stderr_pipe
            rm stdout_pipe stderr_pipe
            
            rename 's/\[/-/; s/\]//' *.csv.zst

            BENCHMARK_FILENAME=$(find . -maxdepth 1 -type f -name "*.csv.zst" -printf "%f\n")
            ./warp analyze --analyze.v ${BENCHMARK_FILENAME} --analyze.out=${BENCHMARK_BUCKET_NAME}_$(date +%s)_analyze.csv > ${BENCHMARK_BUCKET_NAME}_$(date +%s)_analyze.log 2>&1

            echo $(gcloud auth print-access-token) > token.txt
            gcloud config set storage/parallel_composite_upload_enabled False
            gcloud storage --access-token-file=token.txt cp *.zst ${BENCHMARK_BUCKET_PREFIX}${BENCHMARK_BUCKET_NAME}/
            gcloud storage --access-token-file=token.txt cp *.csv ${BENCHMARK_BUCKET_PREFIX}${BENCHMARK_BUCKET_NAME}/
            gcloud storage --access-token-file=token.txt cp *.log ${BENCHMARK_BUCKET_PREFIX}${BENCHMARK_BUCKET_NAME}/

            echo "Benchmark complete and results uploaded to ${BENCHMARK_BUCKET_PREFIX}${BENCHMARK_BUCKET_NAME}"
      restartPolicy: Never
  backoffLimit: 1